\documentclass[11pt]{article}

\usepackage[dvips]{graphicx}
\usepackage{rotating}

\usepackage{freerbmt09}
\usepackage[utf8x]{inputenc}
\usepackage{times}
\usepackage{natbib}
\usepackage{url}
\usepackage{latexsym}

\title{Leveraging Service-Oriented Architectures through efficient and scalable Machine Translation services}

\author{Pasquale Minervini\\
  Dipartimento di Informatica \\
  Universit√† degli Studi di Bari \\
  Via E. Orabona 4, 70125 Bari, Italy \\
  {\tt p.minervini@gmail.com}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Service Oriented Architecture (SOA) is a paradigm for organizing and utilizing distributed services that may be under the 
control of different ownership domains and implemented using various technology stacks. In some contexts, an organization
using an IT infrastructure implementing the SOA paradigm can take a great benefit from an efficient Machine Translation (MT) service. 
This paper describes the architecture used to develop a Machine Translation service that is efficient, scalable and easy to 
integrate in new and existing business processes; our service relies on Apertium, a Free/Open-Source Rule-Based Machine Translation Platform.
\end{abstract}


\section{Introduction}

Service Oriented Architecture is an architectural paradigm providing  a set of principles of governing concepts used during phases 
of systems development and integration. In such an architecture functionalities are packaged as interoperable, loosely coupled
services that may be used to build infrastructures enabling those with needs (consumers) and those with capabilities (providers) 
to interact across different domains of technology and ownership.

Several new trends in the computer industry rely upon SOA as the enabling foundation, including the automation of Business Process 
Management (BPM) and the multitude of new architecture and design patterns generally referred to as Web 2.0~\citep{web20}.\\

In some contexts, an organization could take a great benefit by integrating a MT service in its IT infrastructure to overcome 
language barriers; for example, a common problem arising when building a Knowledge Base starting from a corpus of free text for
a certain domain (like biomedical, financial etc.) is that the text isn't always in a language that can be comprehended by the
domain experts and/or the knowledge extraction tools beign used.

A possible solution to this problem consists in using a Machine
Translation service (eventually using domain-specific dictionaries, rules etc.) to translate the free text from a language to
another with an high accuracy, so that it's then possible to start extracting knowledge from it.\\

{\bf Example:} MetaMap~\citep{metamap} is an application that allows mapping text to UMLS$^{\mbox{\small\textregistered}}$ 
Metathesaurus$^{\mbox{\small\textregistered}}$\footnote{The UMLS$^{\mbox{\small\textregistered}}$ Metathesaurus$^{\mbox{\small\textregistered}}$~\citep{umls}, 
the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classified by semantic 
type and both hierarchical and non-hierarchical relationships among the concepts.} concepts, which have proved to be useful for many applications, 
including decision support systems, management of patient records, information retrieval and data mining within the biomedical domain.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=7.5cm]{mtsoa}
\end{center}
\caption{Representation of a business process in which a clinical document, if written in a language different than English, is fist translated
  to English and then processed using MetaMap to extract UMLS$^{\mbox{\small\textregistered}}$ concepts.}
\label{fig:mtsoa}
\end{figure}

Currently MetaMap is only available for English free text, which makes difficult the use of UMLS$^{\mbox{\small\textregistered}}$ 
Metathesaurus$^{\mbox{\small\textregistered}}$ to represent concepts from biomedical texts written in languages different than English.
A possible way to overcome this limitation consists in using RBMT techniques (possibly with dictionaries, translation rules, 
lexical selection techniques etc. specific for the biomedical domain) to translate the free text from its original language 
to English, and then process it as in Figure \ref{fig:mtsoa}. This approach is also discussed in \shortcite{metamapes}.\\

We realized a prototype service relying on Apertium~\citep{armentano05p}, a free/open-source machine translation platform being 
developed with funding from the Spanish government and the government of Catalonia at the Universitat d'Alacant (University of Alicante), 
for its translation capabilities, and on N-Gram Based Text Categorization~\citep{textcat} for language detection.


\section{Exposing the Service}

Service's interface should provide access to the following capabilities:

\begin{description}
  \item[Translation], for automatic translation of free text from a source language to a destination language;
  \item[Language Detection], for automatic language guessing of free text;
\end{description}

In SOA, interoperability between services is achieved by using standard languages for the description of service interfaces and the communications
among services. A widely accepted technology for implementing SOA consists in using a technology called Web Services~\citep{soa}, defined by the W3C
as defined by the W3C as ``a software system designed to support interoperable machine-to-machine interaction over a network. It has an interface 
described in a machine-processable format (specifically WSDL). Other systems interact with the Web service in a manner prescribed by its description 
using SOAP-messages, typically conveyed using HTTP with an XML serialization in conjunction with other Web-related standards.''~\citep{wsgloss}. An
alternative to SOAP is XML-RPC~\citep{xmlrpcspec}, a remote procedure call protocol which uses XML to encode its calls and HTTP as a transport 
mechanism.\\

For our service, we decided to provide XML-RPC, SOAP and REST interfaces.


\section{Service's Internals}

A \emph{Translation Process} can be considered as the following sequence of steps:

\begin{description}
 \item[Decoding] the meaning of the source text;
 \item[Re-Encoding] this meaning in the destination language.
\end{description}

Behind this apparently simple procedure, there's a complex cognitive operation: to decode the meaning of some free text in its entirety, the translator
needs to interpret and analyse all the features present in the text, which requires in-depth knowledge of \emph{grammar}, \emph{semantics}, \emph{syntax},
\emph{idioms} etc. of both source and destination languages and the culture of their speakers.\\

For this reason, according to \citep{arnoldea} often Machine Translation engines make use, during the translation process, of 
\emph{intermediate representations} (such as an \emph{interlingua}) trying to capture the ``meaning'' of the original sentence in order to generate the 
correct translation. 
In general, systems following those criteria, apply a set of \emph{linguistic rules} during the translation process, which are defined as correspondances 
between the structure of the source language and the structure of the destination language. Therefore, the translation is obtained by analysing the source 
text for morphology and syntax (and eventually semantics) to create an intermediate representation, and then by using bilingual dictionaries and grammatical
rules.\\

Specifically, Apertium uses Finite-State Transducers for lexical processing, Hidden Markov Models for Part-of-Speech tagging and Finite-State-Based Chunking
for structural transfer. Its translation engine consists of an \emph{assembly line}, composed by the following modules: 

\begin{description}
 \item[Formatters], which handles format-specific information respect to text to be translated;
 \item[Morphological Analyser], which tokenizes the text in \emph{surface forms} and delivers, for each surface form, one or more \emph{lexical forms}
  consisting of lemma, lexical category and informations about morphological inflection;
 \item[Part-of-Speech Tagger], which chooses one of the analyses of an ambiguous word, according to its context;
 \item[Lexical Transfer Module], which reads each lexical form of the surface form and delivers the corrisponding destination language lexical form;
 \item[Structural Transfer Module], which detects and processes patterns of words that need special processing due to grammatical divergences between two
  languages;
 \item[Morphological Generator] that, from a lexical form in the destination language, generates a suitably inflected surface form;
 \item[Post-Generator], that performs some orthographic operations in the destination language such as contractions;
\end{description}

In general, those modules often rely on resources whose acquisition and release is expensive: for example, production rule systems relying on \emph{RETE}-like 
pattern matching algorithms~\citep{forgy} build a network of nodes during the acquisition of a set of rules, and this process has usually a relatively
elevated computational cost; in addition, resources as dictionaries are often mapped, during their acquisition, to reduce the
computational complexity of lookup operations,  to hash tables, which also are computationally expensive, in time, to create (because of the repeated use of
the hashing functions, collision handling etc.).\\

To prevent the frequent acquisition and release of the just cited resources, our service makes use of the \emph{Pooling Pattern}~\citep{kircher2001},



\begin{figure}[!ht]
\begin{center}
\includegraphics[width=7.5cm]{resource_pool}
\end{center}
\caption{Object Pool}
\label{fig:rp}
\end{figure}

\section*{Acknowledgements}

Development for this project was funded as part of the Google Summer of Code\footnote{\url{http://code.google.com/soc/}} programme. 
In addition, many thanks go to Jimmy O'Regan, Francis Tyers and others involved in The Apertium Project, for their constant help.

\bibliographystyle{apalike}
\bibliography{freerbmt09}

\end{document}
