
ID	Title	Description	
1	Categorise words by part-of-speech	The objective of this task is to take a frequency list of words for AAA and categorise them by their part-of-speech. Further information can be found on the following page: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Categorise_words_from_frequency_list . You will need to have some knowledge of the language whose words you intend to categorise	
2	Lemmatise words by frequency	The objective of this task is to take a frequency list of words for AAA which have been categorised and then lemmatise them. This means finding the "base" or "dictionary" form of the word. Further information can be found on the following page: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Lemmatise_words_from_frequency_list . You will need to have some knowledge of the language whose words you intend to lemmatise.	
3	Intersection of ATT format transducers 	The objective of these tasks is to write code to intersect two finite-state transducers. The first transducer is a bilingual dictionary that has been converted into prefixes and the second transducer is the monolingual dictionary. You can find out more about the task on this web page: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Intersection_of_ATT_format_transducers	
4	Categorise words by inflectional paradigm	The objective of this task is to find the inflectional paradigm for a series of words in AAA. This can be done manually, or semi-automatically. The following page has further details: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Add_words_from_frequency_list	
5	Extract (scrape) inflections from Wiktionary	The objective of this task is to extract (or scrape) inflectional information for AAA from Wiktionary. This will involve either processing wiki code or HTML code. Further information can  be found at the following link: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Scrape_inflection_information_from_Wiktionary	
6	Language detection in apertium-apy	The objective of this task is to add a function to do language detection to apertium-apy, a python based web services API for Apertium. Further information at the following URL: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Language_detection_in_simple-html_and_apertium-apy	
7	Localised 'available languages' in apertium-apy	The objective of this task is to make a new function to return a list of available languages in translation in apertium-apy, a python-based web-services API for Apertium. Further information can be found at the following URL: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Language_detection_in_simple-html_and_apertium-apy	
8	Hand-annotate 500 words of text	The objective of this task is to manually morphologically disambiguate running words of text in AAA. More information can be found on the Wiki here: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Morphologically_disambiguating_text	
9	Abstract formatting for the simple-html interface	 The <a href="http://wiki.apertium.org/wiki/Simple-html">simple-html</a> interface should be easily customisable so that people can make it look how they want. The task is to abstract the formatting and make one or more new stylesheets to change the appearance. This is basically making a way of "skinning" the interface. 	
10	Interface behaviour for language guessing	The objective of this task is to improve the behaviour of the interface for selecting languages in the simple-html interface for Apertium. It will involve using JavaScript and CSS to choose the most probable language and to highlight other probable languages. Further information can be found here: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Language_detection_in_simple-html_and_apertium-apy	
11	Use external language detection in simple-html	 Make <a href="http://wiki.apertium.org/wiki/Simple-html">simple-html</a> interface not use 2.9MB javascript module for language detection/identification.  Instead it should query <a href="http://wiki.apertium.org/wiki/Apertium-apy" title="Apertium-apy">apertium-apy</a> with text to get a list of languages with probabilities. 	
12	Wiktionary language-page count calculator	The objective of this task is to write a script which calculates the number of pages in a Wiktionary category for a language. The code should be modular, able to work with any given Wiktionary, and should return e.g. the number of pages in Category:Faroese_language. 	
13	Bison syntax-tree visualisation	Write a program which reads a grammar using bison, parses a sentence and outputs the syntax tree as text, or graphViz or something. You can find more information here: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Syntax_tree_printing_in_bison	
14	Document how to install WikiBhasha		
15	Intersection of two transducers in lttoolbox	The objective of this tasks is to write code in C++ using the lttoolbox library to intersect two finite-state transducers. The first transducer is a bilingual dictionary that has been converted into prefixes and the second transducer is the monolingual dictionary. You can find out more about the task on this web page: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Intersection_of_ATT_format_transducers	
16	Front end to intersection in lttoolbox (lt-trim)	The objective of this task is to write a C++ program that provides a front end to intersect two transducers. You can find out more at: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Intersection_of_ATT_format_transducers	
17	Extract translations from Wiktionary	For language pair AAA and BBB extract all the possible translations from Wiktionary. You could do this with a screenscraping or a databases/wikicode parsing methodology. 	
18	Extract paradigm sketches from a dictionary	The objective of this task is to extract paradigm "sketches" for AAA of the kind found in dictionaries and sort them into the different categories. It will involve text processing and quite a bit of shell scripting or python. For more information see: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Extracting_paradigm_sketches_from_dictionaries	
19	Morphologically analyse and align parallel corpus	The objective of this task is to take a parallel corpus for AAA and BBB and to morphologically analyse it using Apertium and then word-align it, for example using GIZA++. 	
20	Hand-annotate 250 words of text	The objective of this task is to manually morphologically disambiguate 250 words of running words of text in AAA. More information can be found on the Wiki here: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Morphologically_disambiguating_text	
21	Extract parallel corpus from web site	The objective of this task is to download web pages from a bilingual (or multilingual) web site and document align them. This means finding out which pages are translations of other pages.	
22	Sentence align parallel corpus	Given a document-aligned parallel corpus, use a sentence splitter (for example NLTK Punkt) and sentence aligner (for example hunalign) to produce a sentence-aligned parallel corpus.	
23	Hand-correct the spelling of 50 words	The objective of this task is to read through documents (e.g. from Wikipedia) and find and correct spelling errors in them, keeping a note of where you found the error. For further information see: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Hand-correct_spelling_errors	
24	Document how to install a voikko spellchecker for LibreOffice	The objective of this task is to document how to install a spellchecker using libvoikko and LibreOffice for language AAA. For further information, contact your mentor.	
25	Document how to install a voikko spellchecker for libenchant	The objective of this task is to document how to install a spellchecker using libvoikko and libenchant for language AAA. For further information, contact your mentor.	
26	Convert paradigm tables to speling format	The objective of this task is to take a grammar for AAA and copy out (or extract) the paradigms in speling format. A description of the speling format can be found here: http://wiki.apertium.org/wiki/Speling_format. Your mentor will provide you with the grammar.	
27	Check output of a word-aligner to make a bilingual dictionary		
28	Manually create a transfer lexicon from a word list in .dix format	The objective of this task is to manually create a transfer lexicon for AAA and BBB. A transfer lexicon is a file that includes words, their equivalents in another language, and tags describing some kinds of structural transfer that might need to take place. 	
29	Proofread an existing dictionary	The objective of this task is to read through an existing dictionary for AAA and BBB and check that the entries are accurate and complete. If an entry is wrong, you should change it, if it is incomplete you should add the missing entries. Contact your mentor on IRC for a copy of the dictionary.	
30	Extract test sentences from a grammar	The objective of this task is to read through a grammar of AAA in BBB and to extract test sentences. Test sentences are short sentences, of under 10 words which demonstrate grammatical differences. Talk to your mentor for details on how to encode the test sentences.	
31	Document how to set up libreoffice-voikko in Windows		
32	Document how to use WikiBhasha		
33	Write a dictionary-based tokeniser for Asian languages	The objective of this task is to write a tokeniser for AAA. A tokeniser takes a sentence and splits it into words. One of the challenges of building a tokeniser for AAA is that spaces are not used to separate words. The tokeniser will have some generic code for reading and writing output, and will use one or more algorithms to determine how to segment the sentence. Read more about this task here: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Tokenisation_for_spaceless_orthographies	
34	Evaluate tokenisation strategies for a South-Asian language		
			
36	Implement language detection in apertium-apy		
37	Write a review of the available literature on word segmentation 	The objective of this task is to write a review of the available search on word segmentation for AAA. This will involve searching for and reading scientific papers and also looking for available software. Further information on this task at: http://wiki.apertium.org/wiki/Task_ideas_for_Google_Code-in/Tokenisation_for_spaceless_orthographies	
38	Document how to install WikiBhasha		
39	Improve the quality of a language pair by adding 50 words to its vocabulary		
40	Add/correct one structural transfer rule to an existing language pair		
41	 Write 10 lexical selection rules for a language pair already set up with lexical selection		
42	Set up a language pair to use lexical selection and write 5 rules		
43	Write 10 constraint grammar rules to repair part-of-speech tagging errors		
			
	Dictionary conversion		
	Dictionary conversion in python		
			
			
	SSL in apertium-apy		
	libvoikko support for apertium-apy		
	*-morph and *-gener modes for apertium-apy		
	performance tracking in apertium-apy		
	apertium-apy gateway		
	apertium-apy gateway server pool management		
56	test and document init scripts for apertium-apy		
	cronjob to detect a "hang" for apertium-apy		
	make apertium-apy use one lock per pipeline		
	make voikkospell understand apertium stream format input		
	make voikkospell return output in apertium stream format		
	libvoikko support for OS X		
	geriaoueg hover functionality: firefox/iceweasel		
	geriaoueg hover functionality: chrome/chromium		
	geriaoueg language/pair selection: firefox/iceweasel		
	geriaoueg language/pair selection: chrome/chromium		
	geriaoueg lookup code: firefox/iceweasel		
	geriaoueg lookup code: chrome/chromium		
	apertium-apy translation-per-word mode		
	apertium-apy mode for geriaoueg (biltrans in context)		
	make apertium-quality work with python3.3 on all platforms		
	How much of a given sentence pair is explained by Apertium?		
	Compare Apertium with another MT system and improve it		
	Check that the Apertium guide for Windows users still works		
	Installation instructions for missing GNU/Linux distributions or versions		
	Installing Apertium in lightweight GNU/Linux distributions		
	What's difficult about this language pair?		
	Video guide to installation		
	Apertium in 5 slides		
	Improved "Become a language-pair developer" document		
	An entry test for Apertium		
	Write a contrastive grammar		
	The most frequent Romance-to-Romance transfer rules		
	Improve the quality of a language pair by allowing for alternative translations		
	Get bible aligner working (or rewrite it)		
	tesseract interface for apertium languages        		
	Abstract the formatting for the simple-html interface.		
	simple-html spell-checker interface		
	simple-html spell-checker code		
	simple-html morphological analysis/generation interface	 Add an enablable morphology module to the simple-html interface.  Should accept text and display analysis (to be gotten via code in another task) or accept analyses and return text.  Functionality similar to <a rel="nofollow" class="external autonumber" href="http://elx.dlsi.ua.es/~fran/turkic/index.php">[5]</a>, but make interface nicer, and integratable into simple-html 	
	simple-html morphological analysis/generation code	 Add code to simple-html to query morphological analysis/generation function of apertium-apy and return results for interface to deal with accordingly 	
			
			
			
	Light Apertium bootable ISO for small machines	 Using <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/Damn_Small_Linux">Damn Small Linux</a> or <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/SliTaz_GNU/Linux">SliTaz</a> or a similar lightweight GNU/Linux,  produce the minimum-possible bootable live ISO or live USB image that contains the OS, minimum editing facilities, Apertium, and a language pair of your choice. Make sure no package that is not strictly necessary for Apertium to run is included.	
	Apertium in XLIFF workflows	  Write a shell script and (if possible, using the filter definition files found in the documentation) a filter that takes an <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/XLIFF">XLIFF</a> file such as the ones representing a computer-aided translation job and populates with translations of all segments that are not translated, marking them clearly as machine-translated. 	
	Examples of minimum files where an Apertium language pair messes up (X)HTML formatting	  Sometimes, an Apertium language pair takes a valid HTML/XHTML source file but delivers an invalid HTML/XHTML target file, regardless of translation quality. This can usually be blamed on incorrect handling of superblanks in structural transfer rules. The task: (1) select a language pair (2) Install Apertium locally from the Subversion repository; install the language pair; make sure that it works (3) download a series of HTML/XHTML files for testing purposes. Make sure they are valid using an HTML/XHTML validator (4) translate the valid files with the language pair (5) check if the translated files are also valid HTML/XHTML files; select those that aren't (6) find the first source of non-validity and study it, and strip the source file until you just have a small (valid!) source file with some text around the minimum possible example of problematic tags; save each such file and describe the error. 	
	Make sure an Apertium language pair does not mess up (X)HTML formatting	 (Depends on someone having performed the task 'Examples of files where an Apertium language pair messes up (X)HTML formatting' above).  The task: (1) run the file through Apertium try to identify where the tags are broken or lost: this is most likely to happen in a structural transfer step; try to identify the rule where the label is broken or lost (2) repair the rule: a conservative strategy is to make sure that all superblanks (&lt;b pos="..."/&gt;) are output and are in the same order as in the source file. This may involve introducing new simple blanks (&lt;b/&gt;) and advancing the output of the superblanks coming from the source. (3) test again (4) Submit a patch to your mentor (or commit it if you have already gained developer access) 	
	Examples of minimum files where an Apertium language pair messes up wordprocessor formatting	  Sometimes, an Apertium language pair takes a valid ODT or RTF source file but delivers an invalid HTML/XHTML target file, regardless of translation quality. This can usually be blamed on incorrect handling of superblanks in structural transfer rules. The task: (1) select a language pair (2) Install Apertium locally from the Subversion repository; install the language pair; make sure that it works (3) download a series of ODT or RTF files for testing purposes. Make sure they are opened using LibreOffice/OpenOffice.org (4) translate the valid files with the language pair (5) check if the translated files are also valid ODT or RTF files; select those that aren't (6) find the first source of non-validity and study it, and strip the source file until you just have a small (valid!) source file with some text around the minimum possible example of problematic tags; save each such file and describe the error. 	
	Make sure an Apertium language pair does not mess up wordprocessor (ODT, RTF) formatting	 (Depends on someone having performed the task 'Examples of files where an Apertium language pair messes up wordprocessor formatting' above).  The task: (1) run the file through Apertium try to identify where the tags are broken or lost: this is most likely to happen in a structural transfer step; try to identify the rule where the label is broken or lost (2) repair the rule: a conservative strategy is to make sure that all superblanks (&lt;b pos="..."/&gt;) are output and are in the same order as in the source file. This may involve introducing new simple blanks (&lt;b/&gt;) and advancing the output of the superblanks coming from the source. (3) test again (4) Submit a patch to your mentor (or commit it if you have already gained developer access) 	
	Start a language pair involving Interlingua	 Start a new language pair involving <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/Interlingua">Interlingua</a> using the <a rel="nofollow" class="external text" href="http://wiki.apertium.orghttp://wiki.apertium.org/wiki/Apertium_New_Language_Pair_HOWTO">Apertium new language HOWTO</a>. Interlingua is the second most used "artificial" language, after Esperanto). As Interlingua is basically a Romance language, you can use a Romance language as the other language, and Romance-language dictionaries rules may be easily adapted. Include at least 50 very frequent words (including some grammatical words) and at least one noun--phrase transfer rule in the ia→X direction. 	
	Generating 'machine translation memories'	 Write a shell script and (using the filter definition files found in the documentation) a filter that takes a plain text file, segments it in sentences using the program segment and an <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/Segmentation_Rules_eXchange">SRX</a> specification (which can be borrowed from <a rel="nofollow" class="external text" href="http://www.omegat.org/">OmegaT</a>) and writes a <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/Translation_Memory_eXchange">TMX</a> file in which each segment is paired with its Apertium translation, ready to be used with OmegaT as a "machine translation memory" 	
	scraper for all wiktionary pages in a category	 a script that returns urls of all pages in a wiktionary category recursively (e.g., <a rel="nofollow" class="external free" href="http://en.wiktionary.orghttp://wiki.apertium.org/wiki/Category:Bashkir_nouns">http://en.wiktionary.orghttp://wiki.apertium.org/wiki/Category:Bashkir_nouns</a> should also include pages from <a rel="nofollow" class="external free" href="http://en.wiktionary.orghttp://wiki.apertium.org/wiki/Category:Bashkir_proper_nouns">http://en.wiktionary.orghttp://wiki.apertium.org/wiki/Category:Bashkir_proper_nouns</a> ) 	
103	 scraper of wiktionary translations between language x and y 	Write  a script that for a given wiktionary page (e.g., <a rel="nofollow" class="external free" href="http://en.wiktionary.orghttp://wiki.apertium.org/wiki/key">http://en.wiktionary.orghttp://wiki.apertium.org/wiki/key</a> ) returns all available translations between two specified languages, with part of speech and meaning/sense for each.	
	better wikipedia extractor script	 Make a single script that performs all the steps listed at <a href="http://wiki.apertium.org/wiki/Wikipedia_Extractor" title="Wikipedia Extractor">Wikipedia Extractor</a>.  That is, it should take a wikipedia dump file as input and output a file that is for all intents and purposes identical to what is output by the last step listed on the wiki.  There should be no intermediate files stored anywhere, and it should not use any more memory than absolutely necessary, but feel free to use as much of the existing code as you need.  You may wish to consult guampa's [much-improved] fork of the WikiExtractor script at <a rel="nofollow" class="external autonumber" href="https://github.com/hltdi/guampa/tree/master/wikipedia-import">[6]</a>, though it doesn't do everything itself either. 	
105	Document materials for a language not yet on our wiki	 Document materials for a language not yet on our wiki.  This should look something like the page on <a href="http://wiki.apertium.org/wiki/Aromanian" title="Aromanian">Aromanian</a>—i.e., all available dictionaries, grammars, corpora, machine translators, etc., print or digital, where available, whether Free, etc., as well as some scholarly articles regarding the language, especially if about computational resources. 	
106	Tag and align a corpus		
	Write a program to convert the Giellatekno Faroese CG to Apertium tags	 Write a program which converts the tagset of the Giellatekno Faroese constraint grammar. 	
			
			
			
	Apertium plugin for WikiBhasha	 Make a plugin for <a rel="nofollow" class="external text" href="http://www.mediawiki.orghttp://wiki.apertium.org/wiki/Extension:WikiBhasha">WikiBhasha</a> that can be used to translate content using the <a href="http://wiki.apertium.org/wiki/API" title="API" class="mw-redirect">apertium API</a> (or <a href="http://wiki.apertium.org/wiki/Apertium-apy" title="Apertium-apy">apertium-apy</a>), with a way to specify the API url to use in a configuration option. 	
	Make WikiBhasha take content from any language's wikipedia	 Modify the code of <a rel="nofollow" class="external text" href="http://www.mediawiki.orghttp://wiki.apertium.org/wiki/Extension:WikiBhasha">WikiBhasha</a> so that it can use ("collect") content from an arbitrary language's wikipedia.  Currently it only takes data from the English-language wikipedia. 	
		 Write a script which takes <a href="http://wiki.apertium.org/wiki/GIZA%2B%2B" title="GIZA++" class="mw-redirect">GIZA++</a> alignments and outputs a <code>.dix</code> file. The script should be able to reduce the number of tags, and also have some heuristics to test if a word is too-frequently aligned. 	
114	Scraper for freely available forum content	 Write a script to scrape/capture all freely available content for a forum or forum category and dump it to an xml corpus file or text file. 	
	Investigate how orthographic modes on kk.wikipedia.org are implemented	 <a rel="nofollow" class="external text" href="http://kk.wikipedia.org">The Kazakh-language wikipedia</a> has a menu at the top for selecting alphabet (Кирил, Latın, توتە - for Cyrillic-, Latin-, and Arabic-script modes).  This appears to be some sort of plugin that transliterates the text on the fly.  Find out what it is and how it works, and then document it somewhere on the wiki.  If this has already been documented elsewhere, point a link to that, but you still should summarise in your own words what exactly it is. 	
	Write a transliteration plugin for mediawiki	 Write a plugin similar in functionality (and perhaps implementation) to the way the <a rel="nofollow" class="external text" href="http://kk.wikipedia.org">Kazakh-language wikipedia</a>'s orthography changing system works.  It should be able to be directed to use any arbitrary mode from an apertium mode file installed in a pre-specified path on a server.	
			
			
	Generalise phenny/begiak git plugin	 Rename the module to <a href="http://wiki.apertium.org/wiki/Begiak">begiak</a> git (instead of github), and test it to make sure it's general enough for at least three common git services (should already be supported, but double check) 	
	phenny/begiak git plugin commit info function	 Add a function to <a href="http://wiki.apertium.org/wiki/Begiak">begiak</a> to get the status of a commit by reponame and name (similar to what the svn module does), and then find out why commit 6a54157b89aee88511a260a849f104ae546e3a65 in turkiccorpora resulted in the following output, and fix it: Something went wrong: dict_keys(['commits', 'user', 'canon_url', 'repository', 'truncated']) 	
	phenny/begiak git plugin recent function	 Find out why the <a href="http://wiki.apertium.org/wiki/Begiak">begiak</a> git recent function (begiak: recent) returns "urllib.error.HTTPError: HTTP Error 401: UNAUTHORIZED (file "/usr/lib/python3.1/urllib/request.py", line 476, in http_error_default)" for one of the repos and fix it so it returns status instead. 	
	phenny/begiak git plugin status	" Add a function that lets anyone (not just admin) get the status of the git event server for <a href=""http://wiki.apertium.org/wiki/Begiak"">begiak</a>
."	
	Document phenny/begiak git plugin	 Document the <a href="http://wiki.apertium.org/wiki/Begiak">begiak</a> git module: how to use it with each service it supports, and the various ways the module can be interacted with (by administrators and anyone) 	
	phenny/begiak svn plugin info function	Find out why the info function in <a href="http://wiki.apertium.org/wiki/Begiak">begiak</a> ("begiak info [repo] [rev]") doesn't work and fix it.	
125	train tesseract on a language with no available tesseract data	 Train tesseract (the OCR software) on a language that it hasn't previously been trained on.  We're especially interested in languages with some coverage in apertium.  We can provide images of text to train on. 	
126	scrape a freely available dictionary using tesseract	 Use tesseract to scrape a freely available dictionary that exists in some image format (pdf, djvu, etc.).  Be sure to scrape grammatical information if available, as well stems (e.g., some dictionaries might provide entries like АЗНА·Х, where the stem is азна), and all possible translations.  Ideally it should dump into something resembling <a href="http://wiki.apertium.org/wiki/Bidix" title="Bidix" class="mw-redirect">bidix</a> format, but if there's no grammatical information and no way to guess at it, some flat machine-readable format is fine. 	
	make scraper plugin for azadliq.org	 Using the directions at <a href="http://wiki.apertium.org/wiki/Writing_a_scraper" title="Writing a scraper">Writing a scraper</a>, make a RFERL scraper for azadliq.org , as a file that loops through stuff like scrp-* file and a class to be included in the scraper_classes file. 	
	make RFERL scraper documentation read like a HOWTO guide	 Enhance the documentation at <a href="http://wiki.apertium.org/wiki/Writing_a_scraper" title="Writing a scraper">Writing a scraper</a> to read/flow more like a HOWTO guide, keeping the current documentation (cleaning/reorganising as needed) as more of a reference guide.  It should include information about what needs to be done for RFERL content and non-RFERL content (more general) 	
	Write an sentence aligner for the UDHR	 Write a script to align two translations of the <a href="http://wiki.apertium.org/wiki/UDHR" title="UDHR">UDHR</a> (final destination: trunk/apertium-tools/udhr_aligner.py).  It should take two UDHR translations and output a tmx file with one article per entry.  It should use the xml formatted UDHRs available from <a rel="nofollow" class="external free" href="http://www.unicode.org/udhr/index_by_name.html">http://www.unicode.org/udhr/index_by_name.html</a> as input and output the aligned texts in tmx format. 	
			
			
			
			
			
	write a browser interface for the concordancer	 Make an html/javascript interface/front-end for <a rel="nofollow" class="external text" href="http://pastebin.com/raw.php?i=KG8ydLPZ">spectie's concordancer</a>, meant for local use.  It should have a box to specify the local path of a corpus, as well as a search box.  Results should be returned without reloading the page.  Ideally, you should use something like <a rel="nofollow" class="external text" href="http://bottlepy.org/">bottlepy</a> for this, and convert the concordancer for use as a library that the server can load.  Make it streamlined both in look and speed (no bloat, please), and make it load results dynamically (via AJAX or similar). 	
	make concordancer work with output of analyser	 Allow <a rel="nofollow" class="external text" href="http://pastebin.com/raw.php?i=KG8ydLPZ">spectie's concordancer</a> to accept an optional apertium mode and directory (implement via argparse).  When it has these, it should run the corpus through that apertium mode and search against the resulting tags and lemmas as well as the surface forms.  E.g., the form алдым might have the analysis via an apertium mode of ^алдым/алд<code>&lt;n&gt;&lt;px1sg&gt;</code><code>&lt;nom&gt;</code>/ал<code>&lt;v&gt;&lt;tv&gt;</code><code>&lt;ifi&gt;&lt;p1&gt;</code><code>&lt;sg&gt;</code>, so a search for "px1sg" should bring up this word. 	
	regex searching in concordancer	 Support searches of regexes, e.g. ".*[кгқғ][ае]н$", in <a rel="nofollow" class="external text" href="http://pastebin.com/raw.php?i=KG8ydLPZ">spectie's concordancer</a>. 	
	scraper for all text urls from kumukia.ru/adabiat	 Write a scraper that gets the urls of all texts at kumukia.ru/adabiat .  It should look into all categories on the navigation bar at the left, and get all urls to texts from each target page. 	
	scraper for all article urls from kumukia.ru/cat-qumuq.html	 Write a scraper that recursively gets the urls of all the articles in the categories linked to at kumukia.ru/cat-qumuq.html .  It should load each page in each category, and get the link to each article (at the text "далее..." = "further..."). 	
	write a scraper plugin for kumukia.ru/adabiat texts       	 <a href="http://wiki.apertium.org/wiki/Writing_a_scraper" title="Writing a scraper">Write a scraper plugin</a> that extracts the raw text from the texts linked to from kumukia.ru/adabiat . 	
	write a scraper plugin for kumukia.ru/cat-qumuq.html articles	  <a href="http://wiki.apertium.org/wiki/Writing_a_scraper" title="Writing a scraper">Write a scraper plugin</a> that extracts the raw text from the articles linked to from kumukia.ru/cat-qumuq.html . 	
	apache forwarding to apertium-apy	 Find out how to get apache to forward an arbitrary url (e.g., <a rel="nofollow" class="external free" href="http://example.com/UrlForAPI">http://example.com/UrlForAPI</a>) to <a href="http://wiki.apertium.org/wiki/Apertium-apy" title="Apertium-apy">apertium-apy</a>, which is a stand-alone service that runs on an arbitrary port.  Document in detail on the wiki. 	
	fix pairviewer's 2- and 3-letter code conflation problems	 <a href="http://wiki.apertium.org/wiki/Pairviewer" title="Pairviewer">pairviewer</a> doesn't always conflate languages that have two codes.  E.g. sv/swe, nb/nob, de/deu, da/dan, uk/ukr, et/est, nl/nld, he/heb, ar/ara, eus/eu are each two separate nodes, but should instead each be collapsed into one node.  Figure out why this isn't happening and fix it.  Also, implement an algorithm to generate 2-to-3-letter mappings for available languages based on having the identical language name in languages.json instead of loading the huge list from codes.json; try to make this as processor- and memory-efficient as possible. 	
	add language searching to pairviewer	 Add some way to search for language (by name or code) to <a href="http://wiki.apertium.org/wiki/Pairviewer" title="Pairviewer">pairviewer</a>.  When your search is matched, it should highlight the node in some clever way (e.g., how OS X dims everything but what you searched for). 	
	come up with better colours for pairviewer	 <a href="http://wiki.apertium.org/wiki/Pairviewer" title="Pairviewer">Pairviewer</a> currently has a set of colours that's semantically annoying.  Ideally we would have something clearer.  The main idea is that the darker colours represent more-worked-on pairs, and "good" colours (e.g., green) represent more production-ready pairs.  The current set of colour scales, instead of relying solely on darkness of colour within each scale, rely on hue also.  Make it so that these scales internally rely more exclusively on darkness.  Try a few different variants and run them by your mentor, who will have final say as to what's best.  (And if you can leave a few variants around that can be switched out easily in the code, that would be good too.)  You can merge the 1-9 and 10-99 categories if you want (so you'll only need 4 shades per hue); anything under 100 stems is a very small language pair, and we don't need any more detail than that. 	
	map support for pairviewer ("pairmapper")	 Write a version of <a href="http://wiki.apertium.org/wiki/Pairviewer" title="Pairviewer">pairviewer</a> that instead of connecting floating nodes, connects nodes on a map.  I.e., it should plot the nodes to an interactive world map (only for languages whose coordinates are provided, in e.g. GeoJSON format), and then connect them with straight-lines (as opposed to the current curved lines).  Use an open map framework, like <a rel="nofollow" class="external text" href="http://leafletjs.com">leaflet</a>, <a rel="nofollow" class="external text" href="http://polymaps.org">polymaps</a>, or <a rel="nofollow" class="external text" href="http://openlayers.org">openlayers</a> 	
	coordinates for Caucasian languages	 Using the map <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Caucasus-ethnic_en.svg">Caucasus-ethnic_en.svg</a>, write a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format that can be loaded by pairmapper (or, e.g., converted to kml and loaded in google maps).  The file should contain points that are a geographic "center" (locus) for where each language on that map is spoken.  Exclude Kurdish (since it's off the map), and keep Azeri in Azerbaycan and Iran separate.  You can use a capital city for bigger, national languages if you'd like (think Paris as a locus for French). 	
	coordinates for Central Asian languages	 Using the map <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Central_Asia_Ethnic_en.svg">Central_Asia_Ethnic_en.svg</a>, write a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format that can be loaded by pairmapper (or, e.g., converted to kml and loaded in google maps).  The file should contain points that are a geographic "center" (locus) for where each Turkic language on that map (and also Tajik) is spoken.  You can use a capital city for bigger, national languages if you'd like (think Paris as a locus for French). 	
	coordinates for Mongolic languages	 Using the map <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/File:Linguistic_map_of_the_Mongolic_languages.png">Linguistic map of the Mongolic languages.png</a>, write a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format that can be loaded by pairmapper (or, e.g., converted to kml and loaded in google maps).  The file should contain points that are a geographic "center" (locus) for where each Mongolic language on that map is spoken.  Use the term "Khalkha" (iso 639-3 khk) for "Mongolisch", and find a better map for Buryat.  You can use a capital city for bigger, national languages if you'd like (think Paris as a locus for French). 	
	draw languages as areas for pairmapper	 Make a map interface that loads data (in e.g. GeoJSON or KML format) specifying areas where languages are spoken, as well as a single-point locus for the language, and displays the areas on the map (something like <a rel="nofollow" class="external text" href="http://leafletjs.com/examples/choropleth.html">the way the states are displayed here</a>) with a node with language code (like for <a href="http://wiki.apertium.org/wiki/Pairviewer" title="Pairviewer">pairviewer</a>) at the locus.  This should be able to be integrated into pairmapper, the planned map version of pairviewer. 	
	georeference  language areas for Tatar, Bashqort, and Chuvash	 Using the maps listed here, try to define rough areas for where Tatar, Bashqort, and Chuvash are spoken.  These areas should be specified in a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin.  Try to be fairly accurate and detailed.  Maps to consult include <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Tatarbashkirs1989ru.PNG">Tatarsbashkirs1989ru</a>, <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:NarodaCCCP.jpg">NarodaCCCP</a> 	
	georeference language areas for North Caucasus Turkic languages	 Using the maps listed here, try to define rough areas for where Tatar, Bashqort, and Chuvash are spoken.  These areas should be specified in a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin.  Try to be fairly accurate and detailed.  Maps to consult include <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Tatarbashkirs1989ru.PNG">Tatarsbashkirs1989ru</a>, <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:NarodaCCCP.jpg">NarodaCCCP</a> 	
	georeference language areas for IE and Mongolic Caucasus-area languages	 Using the map <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Caucasus-ethnic_en.svg">Caucasus-ethnic_en.svg</a>, write a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin.  The file should contain specifications for the area(s) the following languages are spoken in: Ossetian, Armenian, Kalmyk.  There should be a certain level of detail (e.g., don't just make a shape matching Kazakhstan for Kazakh) and accuracy (i.e., don't just put a square over Kazakhstan and call it the area for Kazakh). 	
	georeference language areas for North Caucasus languages	 Using the map <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Caucasus-ethnic_en.svg">Caucasus-ethnic_en.svg</a>, write a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin.  The file should contain specifications for the area(s) the following languages are spoken in: Avar, Chechen, Abkhaz, Georgian.  There should be a certain level of detail (e.g., don't just make a shape matching Kazakhstan for Kazakh) and accuracy (i.e., don't just put a square over Kazakhstan and call it the area for Kazakh). 	
	georeference language areas for misc Caucasus-area languages	 Using the maps <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Caucasus-ethnic_en.svg">Caucasus-ethnic_en.svg</a> and <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Lezgin_map.png">Lezgin_map</a>, write a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin.  The file should contain specifications for the area(s) the following languages are spoken in: Lezgi, Azeri (Azerbaycan), Azeri (Iran), Ingush.  There should be a certain level of detail (e.g., don't just make a shape matching Kazakhstan for Kazakh) and accuracy (i.e., don't just put a square over Kazakhstan and call it the area for Kazakh). 	
	georeference language areas for Kazakh	 Using the map <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Central_Asia_Ethnic_en.svg">Central_Asia_Ethnic_en.svg</a>, write a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin.  The file should contain specifications for the areas Kazakh is spoken in, with a certain level of detail (e.g., don't just make a shape matching Kazakhstan for Kazakh) and accuracy (i.e., don't just put a square over Kazakhstan and call it the area for Kazakh). 	
	georeference language areas for Karakalpak and Kyrgyz	 Using the map <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Central_Asia_Ethnic_en.svg">Central_Asia_Ethnic_en.svg</a>, write a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin.  The file should contain specifications for the areas Karakalpak and Kyrgyz are spoken in, with a certain level of detail (e.g., don't just make a shape matching Kazakhstan for Kazakh) and accuracy (i.e., don't just put a square over Kazakhstan and call it the area for Kazakh). 	
	georeference language areas for  Uzbek and Uyghur	 Using the map <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Central_Asia_Ethnic_en.svg">Central_Asia_Ethnic_en.svg</a>, write a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin.  The file should contain specifications for the areas Tajik and Turkmen are spoken in, with a certain level of detail (e.g., don't just make a shape matching Kazakhstan for Kazakh) and accuracy (i.e., don't just put a square over Kazakhstan and call it the area for Kazakh). 	
	georeference language areas for Tajik and Turkmen	 Using the map <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Central_Asia_Ethnic_en.svg">Central_Asia_Ethnic_en.svg</a>, write a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin.  The file should contain specifications for the areas Tajik and Turkmen are spoken in, with a certain level of detail (e.g., don't just make a shape matching Kazakhstan for Kazakh) and accuracy (i.e., don't just put a square over Kazakhstan and call it the area for Kazakh). 	
	georeference language areas for Russian	 Assume areas in Central Asia with any sort of measurable Russian population speak Russian.  Use the following maps to create a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin: <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Kazakhstan_European_2012_Rus.png">Kazakhstan_European_2012_Rus</a>, <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Ethnicrussians1989ru.PNG">Ethnicrussians1989ru</a>, <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Lenguas_eslavas_orientales.PNG">Lenguas_eslavas_orientales</a>, <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:NarodaCCCP.jpg">NarodaCCCP</a>.  Try to cover all the areas where Russian is spoken at least as a major language. 	
	georeference language areas for Ukrainian and Belorussian	 Use the following maps to create a file in <a rel="nofollow" class="external text" href="https://en.wikipedia.orghttp://wiki.apertium.org/wiki/GeoJSON">GeoJSON</a> (or similar) format for use by pairmapper's languages-as-areas plugin that defines where Belorussian and Ukrainian are spoken:  <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:Lenguas_eslavas_orientales.PNG">Lenguas_eslavas_orientales</a>, <a rel="nofollow" class="external text" href="https://commons.wikimedia.orghttp://wiki.apertium.org/wiki/File:NarodaCCCP.jpg">NarodaCCCP</a>. 	
	split nor into nob and nno in pairviewer	 Currently in <a href="http://wiki.apertium.org/wiki/Pairviewer" title="Pairviewer">pairviewer</a>, nor is displayed as a language separately from nob and nno.  However, the nor pair actually consists of both an nob and an nno component.  Figure out a way for pairviewer (or pairsOut.py / get_all_lang_pairs.py) to detect this split.  So instead of having swe-nor, there would be swe-nob and swe-nno displayed (connected seemlessly with other nob-* and nno-* pairs), though the paths between the nodes would each still give information about the swe-nor pair.  Implement a solution, trying to make sure it's future-proof (i.e., will work with similar sorts of things in the future). 	
	add support to pairviewer for regional and alternate orthograpic modes	 Currently in <a href="http://wiki.apertium.org/wiki/Pairviewer" title="Pairviewer">pairviewer</a>, there is no way to detect or display modes like zh_TW.  Add suppor to pairsOut.py / get_all_lang_pairs.py to detect pairs containing abbreviations like this, as well as alternate orthographic modes in pairs (e.g. uzb_Latn and uzb_Cyrl).  Also, figure out a way to display these nicely in the pairviewer's front-end.  Get creative.  I can imagine something like zh_CN and zh_TW nodes that are in some fixed relation to zho (think Mickey Mouse configuration?).  Run some ideas by your mentor and implement what's decided on. 	
	using language transducers for predictive text on Android	 Investigate what it would take to add some sort of plugin to existing Android predictive text / keyboard framework(s?) that would allow the use of lttoolbox (or hfst?  or libvoikko stuff?) transducers to be used to predict text and/or guess swipes (in "swype" or similar).  Document your findings on the apertium wiki. 	
	custom predictive text keyboards for Android	 Research and document on apertium's wiki the steps needed to design an application for Android that could load arbitrarily defined / pre-specified keyboard layouts (e.g., say I want to make custom keyboard layouts for <a href="http://wiki.apertium.org/wiki/Kumyk" title="Kumyk">Kumyk</a> and <a href="http://wiki.apertium.org/wiki/Guaran%C3%AD" title="Guaraní">Guaraní</a>, and load either one into the same program) as well as either an lttoolbox-format transducer or a file easily generated from one that could be paired with a keyboard layout and used to predict text in that language. 	
	phenny/begiak ethnologue plugin	 Make a phenny plugin for use in <a href="http://wiki.apertium.org/wiki/Begiak" title="Begiak">begiak</a> that looks up language names and codes on <a rel="nofollow" class="external text" href="http://www.ethnologue.com">ethnologue</a> and returns basic info matching the search: language name, iso 639-3 code, where spoken, total number of speakers, and url to main page on ethnologue (note that irc text is character-limited, so it should be concise, e.g. "12,500,000 native speakers" could be abbreviated as just "12.5M L1").  This should work kind of like the .wik and .iso639 plugins (feel free to steal code from those—note that the .wik/.awik plugins chop text (rather well) to fit in a single irc message). 	
	phenny/begiak url module localisation improvements	 Phenny/<a href="http://wiki.apertium.org/wiki/Begiak" title="Begiak">begiak</a> has a module that detects pasted urls and reports the title of the page.  This doesn't work well with non-UTF8-encoded webpages.  Fix this so that the titles get properly converted to UTF8 and display as intended.  Some titles to test on include <a rel="nofollow" class="external text" href="http://www.philology.ru/linguistics4/madiyeva-67.htm">Ìàäèåâà - Àâàðñêèé ÿçûê</a>, <a rel="nofollow" class="external text" href="http://ruscorpora.ru/search-para-uk.html">Ïîèñê â êîðïóñå. Íàöèîíàëüíûé êîðïóñ ðóññêîãî ÿçûêà</a>, <a rel="nofollow" class="external text" href="http://www.pravda.com.ua/">Óêðà¿íñüêà ïðàâäà</a>, <a rel="nofollow" class="external text" href="http://hypar.ru/nws/show/32/25/index.php">×óâàøñêàÿ ðåñïóáëèêàíñêàÿ ãàçåòà «Õûïàð»</a>, <a rel="nofollow" class="external text" href="http://aot.ru/docs/rusmorph.html">ÀÎÒ&nbsp;:: Òåõíîëîãèè&nbsp;:: Ðóññêàÿ ìîðôîëîãèÿ</a> 	
	phenny/begiak mediawiki plugin(s) support for subsections	 Add support to the .wik and .awik phenny/<a href="http://wiki.apertium.org/wiki/Begiak" title="Begiak">begiak</a> modules for subsections.  Currently if you do something like ".wik French language#Phonology" it brings up random text from the French_language article; instead it should find the Phonology section and return the first bit of text from it (the same as with just an article). 	
	monodix support for stem-counting script	 Add support to the <a rel="nofollow" class="external text" href="https://svn.code.sf.net/p/apertium/svn/trunk/apertium-tools/get_stems.py">dix stem-counter</a> for <a href="http://wiki.apertium.org/wiki/Monodix" title="Monodix">monodix</a> format.  It currently only works on <a href="http://wiki.apertium.org/wiki/Bidix" title="Bidix" class="mw-redirect">bidix</a> format.  For monodix format, it should count <code>&lt;e&gt;</code> elements that specify lm="something". 	
			
			
			
			
			
			
			
			
			
			
			
